{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import adapters\n",
    "import datasets\n",
    "import evaluate\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BartTokenizer, BartForSequenceClassification, DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AdamW, Seq2SeqTrainer, Seq2SeqTrainingArguments, get_scheduler, Trainer, TrainingArguments, GenerationConfig, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb024cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed Function\n",
    "def set_logging_and_seed(seed=42):\n",
    "    # Logging output settings\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    logger = transformers.utils.logging.get_logger(\"transformers\")\n",
    "    transformers.utils.logging.set_verbosity(30)\n",
    "    logger.warning(\"WARN\")\n",
    "\n",
    "    # Random seet outputs\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if (torch.cuda.is_available()): torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CUDA Availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model/Tokenizer\n",
    "set_logging_and_seed()\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Select the configuration\n",
    "if (model_name == \"mistralai/Mistral-7B-v0.1\"):\n",
    "    final_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Freeze classification head\n",
    "    freeze = True\n",
    "    if (freeze == True):\n",
    "        for param in final_model.model.parameters(): param.requires_grad = False\n",
    "        pass\n",
    "    \n",
    "    # Load tokenizer\n",
    "    injected_tokenizer = AutoTokenizer.from_pretrained(\"mistral-tokenizer\")\n",
    "    \n",
    "    # Add explicit pad token for Mistral\n",
    "    if injected_tokenizer.pad_token is None:\n",
    "        injected_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        final_model.resize_token_embeddings(len(injected_tokenizer))\n",
    "\n",
    "    # Ensure Mistral has a valid pad token ID\n",
    "    final_model.config.pad_token_id = injected_tokenizer.pad_token_id\n",
    "    \n",
    "elif (model_name != \"pier\"):\n",
    "    final_model = BartForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Load tokenizer\n",
    "    injected_tokenizer = BartTokenizer.from_pretrained(\"bart-large-tokenizer\")\n",
    "else:\n",
    "    final_model = BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", num_labels=2)\n",
    "    \n",
    "    # Initialize architecture for PIER+, then load in the weights\n",
    "    adapters.init(final_model)\n",
    "    final_model.load_adapter(\"~/path/\")\n",
    "    final_model.set_active_adapters(\"non-compositional\")\n",
    "    final_model.load_adapter(\"~/path/\")\n",
    "    final_model.set_active_adapters(\"compositional\")\n",
    "    adapter_fusion_name = final_model.load_adapter_fusion(\"~/path/\")\n",
    "    final_model.set_active_adapters(adapter_fusion_name)\n",
    "    final_model.train_adapter(\"compositional\")\n",
    "    final_model.train_adapter(\"non-compositional\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    injected_tokenizer = BartTokenizer.from_pretrained(\"bart-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLUTE\n",
    "flute_train = pd.read_json(path_or_buf=\"~/path/\", lines=True).values.tolist() # 1768\n",
    "flute_test = pd.read_json(path_or_buf=\"~/path/\", lines=True).values.tolist() # 250\n",
    "\n",
    "flute_train_final = []\n",
    "flute_test_final = []\n",
    "\n",
    "for i in range(len(flute_train)):\n",
    "    sample = flute_train[i]\n",
    "    if (sample[-2] == \"Contradiction\"): label = [0.0, 1.0]\n",
    "    else: label = [1.0, 0.0]\n",
    "    flute_train_final.append([sample[1], sample[2], label])\n",
    "    pass\n",
    "\n",
    "for i in range(len(flute_test)):\n",
    "    sample = flute_test[i]\n",
    "    if (sample[-2] == \"Contradiction\"): label = [0.0, 1.0]\n",
    "    else: label = [1.0, 0.0]\n",
    "    flute_test_final.append([sample[1], sample[2], label])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9162e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMPLI\n",
    "set_logging_and_seed()\n",
    "\n",
    "magpie_e = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "magpie_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "magpie_adversarial_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "pie_e = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "pie_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "pie_adversarial_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "semeval_e = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "semeval_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "semeval_adversarial_ne = pd.read_csv(\"~/path/\", sep=\"\\t\").values.tolist()\n",
    "manual_e = pd.read_csv(\"~/path/\", sep=\"\\t\", header=None).values.tolist()\n",
    "manual_ne = pd.read_csv(\"~/path/\", sep=\"\\t\", header=None).values.tolist()\n",
    "manual_antonyms_ne = pd.read_csv(\"~/path/\", sep=\"\\t\", header=None).values.tolist()\n",
    "\n",
    "def label_cleaning(e, ne, adversarial_ne):\n",
    "    for i in range(len(e)): e[i] = [e[i][0], e[i][1], [1.0, 0.0]]\n",
    "    for i in range(len(ne)): ne[i] = [ne[i][0], ne[i][1], [0.0, 1.0]]\n",
    "    for i in range(len(adversarial_ne)): adversarial_ne[i] = [adversarial_ne[i][0], adversarial_ne[i][1], [0.0, 1.0]]\n",
    "    return e, ne, adversarial_ne\n",
    "\n",
    "magpie_e, magpie_ne, magpie_adversarial_ne = label_cleaning(magpie_e, magpie_ne, magpie_adversarial_ne)\n",
    "pie_e, pie_ne, pie_adversarial_ne = label_cleaning(pie_e, pie_ne, pie_adversarial_ne)\n",
    "semeval_e, semeval_ne, semeval_adversarial_ne = label_cleaning(semeval_e, semeval_ne, semeval_adversarial_ne)\n",
    "manual_e, manual_ne, manual_antonyms_ne = label_cleaning(manual_e, manual_ne, manual_antonyms_ne)\n",
    "\n",
    "def length_indicators(data, max_len=64, min_len=None, invalid_labels=[]):   \n",
    "    premi = []\n",
    "    hypothi = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        prem_len, hypoth_len = data[i][0], data[i][1]\n",
    "        prem_len = len(prem_len.split(\" \"))\n",
    "        hypoth_len = len(hypoth_len.split(\" \"))\n",
    "\n",
    "        if (prem_len >= max_len or (min_len != None and prem_len <= min_len)):\n",
    "            premi.append(i)\n",
    "            \n",
    "        if (hypoth_len >= max_len or (min_len != None and hypoth_len <= min_len)):\n",
    "            hypothi.append(i)\n",
    "            \n",
    "        if (magpie_t[i][2] in invalid_labels):\n",
    "            premi.append(i)\n",
    "\n",
    "    invalid_indices = list(set(premi + hypothi))\n",
    "\n",
    "    for index in sorted(invalid_indices, reverse=True):\n",
    "        del data[index]\n",
    "    \n",
    "    return data\n",
    "\n",
    "mlin = None # Toggle\n",
    "magpie_e = length_indicators(magpie_e, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "magpie_ne = length_indicators(magpie_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "magpie_adversarial_ne = length_indicators(magpie_adversarial_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "pie_e = length_indicators(pie_e, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "pie_ne = length_indicators(pie_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "pie_adversarial_ne = length_indicators(pie_adversarial_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "semeval_e = length_indicators(semeval_e, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "semeval_ne = length_indicators(semeval_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])\n",
    "semeval_adversarial_ne = length_indicators(semeval_adversarial_ne, min_len=mlin, verbose=verbosity, invalid_labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e71f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FigurativeNarrativeBenchmark\n",
    "set_logging_and_seed()\n",
    "\n",
    "fnb_train = pd.read_json(path_or_buf=\"~/path/\", lines=True).values.tolist()\n",
    "with open(\"~/path/\", \"rb\") as file: fnb_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Define Text Correction (Toggle Whether to Use)\n",
    "def reformat_text(text):\n",
    "    if (text == \"\"): return text\n",
    "    \n",
    "    text = text.replace(\"  </s>\", \". </s>\")\n",
    "    text = text.replace(\"_\", \" \") \n",
    "    text = text.replace(\"  \", \", \") \n",
    "    text = text.replace(\" i \", \" I \")\n",
    "    text = text.replace(\" , \", \", \") \n",
    "    text = text.replace(\" - \", \"-\") \n",
    "    text = text.replace(\"We 'll\", \"We'll\") \n",
    "    text = text.replace(\",I\", \", I\") \n",
    "    text = text.replace(\"I 'd\", \"I'd\")\n",
    "    text = text.replace(\"I' ll\", \"I'll\") \n",
    "    text = text.replace(\"you 'll\", \"you'll\") \n",
    "    text = text.replace(\"ca n't\", \"can't\") \n",
    "    text = text.replace(\"they 'd\", \"they'd\") \n",
    "    text = text.replace(\"( \", \"(\")\n",
    "    text = text.replace(\" )\", \")\")\n",
    "    text = text.replace(\" ; \", \"; \")\n",
    "    text = text.replace(\"Let 's\", \"Let's\")\n",
    "    text = text.replace(\"â€˜ \", \"\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\" 's\", \"'s\")\n",
    "    text = text.replace(\" 'll\", \"'ll\")\n",
    "    text = text.replace(\"..\", \".\")\n",
    "    text = text.replace(\" . \", \".\")\n",
    "    text = text.replace(\" .\", \".\")    \n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    \n",
    "    # Capitalise sentence\n",
    "    if (len(text) > 1): text = text[0].upper() + text[1:] \n",
    "    else: text = text.upper()\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Add punctuation\n",
    "    if (text[-1] != \".\" and text[-1] != \"!\" and text[-1] != \"?\"): text = text + \".\"\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FigurativeNarrativeBenchmark Auxiliary Methods\n",
    "set_logging_and_seed()\n",
    "\n",
    "def fnb_remove_contexts(premise, amount_keep=None, total_removal=False, remove_idiom=False):\n",
    "    if (total_removal == True):\n",
    "        if (\"<b>\" not in premise): return premise.split(\". \")[-1] # The last sentence, which usually contains the IE\n",
    "        text = premise.split(\"<b>\")\n",
    "        text = text[1]\n",
    "        text = text.split(\"</b>\")\n",
    "        return text[0]\n",
    "    \n",
    "    # Perform a percentage removal of words starting from the beginning\n",
    "    if (amount_keep != None):\n",
    "        all_words = premise.split(\" \")\n",
    "        keep = int(amount_keep * len(all_words))\n",
    "        text = \" \".join(all_words[-keep:])\n",
    "        return text\n",
    "    \n",
    "    # Default: remove all sentences except for the one containing the idiom\n",
    "    text = premise.split(\". \")\n",
    "    idx = -1\n",
    "    \n",
    "    # Determine which sentence the idiom is in\n",
    "    for i in range(len(text)):\n",
    "        if (\"<b>\" in text[i]): idx = i\n",
    "        pass\n",
    "        \n",
    "    return text[idx]\n",
    "\n",
    "def fnb_shuffling(premise):\n",
    "    text = premise.split(\" \")\n",
    "    \n",
    "    # Get the idiom's range of indices\n",
    "    start = 0\n",
    "    end = -1\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        if ((\"<b>\" in text[i] or \"b>\" in text[i] or \"<b\" in text[i]) and (start == 0)): start = i\n",
    "        if ((\"</b>\" in text[i] or \"/b>\" in text[i] or \"</b\" in text[i]) and (end == -1)): end = i\n",
    "        pass\n",
    "    \n",
    "    if (end != len(text) - 1): end += 1\n",
    "    \n",
    "    start_portion = text[:start]\n",
    "    end_portion = text[end:]\n",
    "    \n",
    "    # Shuffle both portions\n",
    "    random.shuffle(start_portion)\n",
    "    if (len(end_portion) > 0): random.shuffle(end_portion)\n",
    "    \n",
    "    # Re-append the two portions\n",
    "    shuffled_text = start_portion + text[start:end] + end_portion\n",
    "    return (\" \".join(shuffled_text)).strip()\n",
    "\n",
    "def fnb_random_removal(premise, amount_keep=0.9):\n",
    "    # Get number of words\n",
    "    text = premise.split(\" \")\n",
    "    num_words = len(text)\n",
    "    num_keep = int(num_words * amount_keep)\n",
    "    num_remove = num_words - num_keep\n",
    "    \n",
    "    # Get the location of the idiom\n",
    "    start = 0\n",
    "    end = -1\n",
    "    for i in range(len(text)):\n",
    "        if ((\"<b>\" in text[i] or \"b>\" in text[i] or \"<b\" in text[i]) and (start == 0)): start = i\n",
    "        \n",
    "        if ((\"</b>\" in text[i] or \"/b>\" in text[i] or \"</b\" in text[i]) and (end == -1)): end = i\n",
    "        pass\n",
    "    \n",
    "    if (end != len(text) - 1): end += 1\n",
    "        \n",
    "    # Get the list of indices to remove\n",
    "    idx = []\n",
    "    for i in range(num_remove):\n",
    "        removal_idx = np.random.randint(num_words)\n",
    "        while (removal_idx >= start and removal_idx < end):\n",
    "            removal_idx = np.random.randint(num_words)\n",
    "        idx.append(removal_idx)\n",
    "        pass\n",
    "    \n",
    "    text = [j for i, j in enumerate(text) if i not in idx]\n",
    "    return \" \".join(text).strip()\n",
    "\n",
    "def fnb_retrieve_real_idiom(premise):\n",
    "    if (\"<b>\" not in premise): return premise.split(\". \")[-1] # The last sentence, which usually contains the IE\n",
    "    text = premise.split(\"<b>\")\n",
    "    text = text[1]\n",
    "    text = text.split(\"</b>\")\n",
    "    return text[0]\n",
    "\n",
    "def generate_gibberish(cutoff=15, s=\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "    length = np.random.randint(cutoff)\n",
    "    \n",
    "    text = \"\"\n",
    "    for i in range(length): text += random.choice(s)\n",
    "    return text\n",
    "\n",
    "# Uncomment below if we replace the IE with a randomly generated string\n",
    "# '''\n",
    "for i in range(len(fnb_test)):\n",
    "    idiom_temp = fnb_retrieve_real_idiom(fnb_test[i][0])\n",
    "    gibb = generate_gibberish()\n",
    "    fnb_test[i][0] = fnb_test[i][0].replace(idiom_temp, gibb)\n",
    "    pass\n",
    "# '''\n",
    "\n",
    "def fnb_split(fnb_train, fnb_test, full_continuation=False, amount_keep=None, total_removal=False, remove_idiom=False, original=False, no_removal=False, shuffle=False, random_removal=False, move_idiom=False):\n",
    "    train_fnb = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    \n",
    "    # Add training samples\n",
    "    for i in range(len(fnb_train)):\n",
    "        sample = fnb_train[i]\n",
    "        truth = sample[5]\n",
    "        \n",
    "        # Append the options as follows\n",
    "        if (truth == \"option1\"):\n",
    "            train_fnb[\"premises\"].append(reformat_text(sample[0]))\n",
    "            train_fnb[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            train_fnb[\"labels\"].append([1.0, 0.0])\n",
    "            \n",
    "            # Append the non-entailment converse sample\n",
    "            train_fnb[\"premises\"].append(reformat_text(sample[0]))\n",
    "            train_fnb[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            train_fnb[\"labels\"].append([0.0, 1.0])\n",
    "        else:\n",
    "            train_fnb[\"premises\"].append(reformat_text(sample[0]))\n",
    "            train_fnb[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            train_fnb[\"labels\"].append([0.0, 1.0])\n",
    "            \n",
    "            # Append the non-entailment converse sample\n",
    "            train_fnb[\"premises\"].append(reformat_text(sample[0]))\n",
    "            train_fnb[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            train_fnb[\"labels\"].append([1.0, 0.0])\n",
    "        pass\n",
    "    \n",
    "    # Now do the same for the test split\n",
    "    test_fnb = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    test_e = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    test_ne = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    \n",
    "    for i in range(len(fnb_test)):\n",
    "        sample = fnb_test[i]\n",
    "        truth = sample[5]\n",
    "        premise = fnb_remove_contexts(sample[0], amount_keep=amount_keep, total_removal=total_removal, remove_idiom=remove_idiom)\n",
    "        \n",
    "        if (no_removal == True):\n",
    "            premise = sample[0]\n",
    "            if (shuffle == True): premise = fnb_shuffling(premise)\n",
    "        if (random_removal == True): premise = fnb_random_removal(premise, amount_keep=amount_keep)\n",
    "            \n",
    "        premise = reformat_text(premise)\n",
    "        \n",
    "        # Append the options as follows\n",
    "        if (truth == \"option1\"):\n",
    "            test_fnb[\"premises\"].append(premise)\n",
    "            test_fnb[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            test_fnb[\"labels\"].append([1.0, 0.0])\n",
    "            \n",
    "            test_e[\"premises\"].append(premise)\n",
    "            test_e[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            test_e[\"labels\"].append([1.0, 0.0])\n",
    "            \n",
    "            test_fnb[\"premises\"].append(premise)\n",
    "            test_fnb[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            test_fnb[\"labels\"].append([0.0, 1.0])\n",
    "            \n",
    "            test_ne[\"premises\"].append(premise)\n",
    "            test_ne[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            test_ne[\"labels\"].append([0.0, 1.0])\n",
    "        else:\n",
    "            test_fnb[\"premises\"].append(premise)\n",
    "            test_fnb[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            test_fnb[\"labels\"].append([0.0, 1.0])\n",
    "            \n",
    "            test_ne[\"premises\"].append(premise)\n",
    "            test_ne[\"hypotheses\"].append(reformat_text(sample[3]))\n",
    "            test_ne[\"labels\"].append([0.0, 1.0])\n",
    "            \n",
    "            test_fnb[\"premises\"].append(premise)\n",
    "            test_fnb[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            test_fnb[\"labels\"].append([1.0, 0.0])\n",
    "            \n",
    "            test_e[\"premises\"].append(premise)\n",
    "            test_e[\"hypotheses\"].append(reformat_text(sample[4]))\n",
    "            test_e[\"labels\"].append([1.0, 0.0])        \n",
    "        \n",
    "    # Convert all the data to Datasets\n",
    "    train_fnb = Dataset.from_dict(train_fnb).shuffle(seed=42)\n",
    "    test_fnb = Dataset.from_dict(test_fnb)\n",
    "    test_e = Dataset.from_dict(test_e)\n",
    "    test_ne = Dataset.from_dict(test_ne)\n",
    "    test_antonyms = Dataset.from_dict({\"premises\" : [], \"hypotheses\" : [], \"labels\" : []})\n",
    "    \n",
    "    return train_fnb, test_fnb, test_e, test_ne, test_antonyms\n",
    "\n",
    "train_fnb_dataset, test_fnb_dataset, test_fnb_manual_e, test_fnb_manual_ne, test_fnb_manual_antonyms_ne = fnb_split(fnb_train, \n",
    "                                                                                                                    fnb_test, \n",
    "                                                                                                                    full_continuation=False,\n",
    "                                                                                                                    amount_keep=None,\n",
    "                                                                                                                    total_removal=False,\n",
    "                                                                                                                    no_removal=False,\n",
    "                                                                                                                    shuffle=False,\n",
    "                                                                                                                    random_removal=False)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premises = examples[\"premises\"]\n",
    "    hypotheses = examples[\"hypotheses\"]\n",
    "    \n",
    "    return injected_tokenizer(premises, hypotheses, truncation=True)\n",
    "\n",
    "encoded_fnb_train = train_fnb_dataset.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_fnb_test = test_fnb_dataset.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "\n",
    "encoded_fnb_manual_e = test_fnb_manual_e.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_fnb_manual_ne = test_fnb_manual_ne.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=injected_tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f21b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLI Auxiliary Methods\n",
    "set_logging_and_seed()\n",
    "\n",
    "def remove_contexts(t1, t2):\n",
    "    # Forward pass\n",
    "    s1 = t1.split(\" \")\n",
    "    s2 = t2.split(\" \")\n",
    "    min_len = min(len(s1), len(s2))\n",
    "    l = -1 # Left cutoff\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if (s1[i].lower() != s2[i].lower()):\n",
    "            l = i\n",
    "            break\n",
    "        pass\n",
    "    \n",
    "    # Cutoff the differing portion\n",
    "    s1 = s1[l:]\n",
    "    s2 = s2[l:]\n",
    "        \n",
    "    # Backwards pass\n",
    "    min_len = min(len(s1), len(s2))\n",
    "    r = -10\n",
    "    \n",
    "    for i in range(1, min_len):\n",
    "        if (s1[-i].lower() != s2[-i].lower()):\n",
    "            r = -i\n",
    "            break\n",
    "        pass\n",
    "    if (min_len == 1): r = -1\n",
    "        \n",
    "    # If we reached the end of the for loop with no differences, keep at least one word\n",
    "    if (r == -10):\n",
    "        r = min_len\n",
    "        s1 = s1[:-r + 1]\n",
    "        s2 = s2[:-r + 1]\n",
    "    \n",
    "    # If we have a normal cutoff then just use that of course\n",
    "    elif (r != -1):        \n",
    "        s1 = s1[:r+1]\n",
    "        s2 = s2[:r+1]\n",
    "    \n",
    "    return \" \".join(s1), \" \".join(s2)\n",
    "\n",
    "# Retrieve dataframe of IMPLI dataset and corresponding model predictions, as well as other data\n",
    "with open(\"~/path/\", \"rb\") as file:\n",
    "    impli_df = pickle.load(file)\n",
    "    \n",
    "def impli_shuffling(premise, idx):\n",
    "    idiom = impli_df.iloc[idx][\"idiom\"].split(\" \")\n",
    "    text = premise.split(\" \")\n",
    "    completed = False\n",
    "    ii = 0 # The idiom traversal index\n",
    "    start = 0\n",
    "    end = -1\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        score = fuzz.ratio(text[i].lower(), idiom[ii].lower())\n",
    "        \n",
    "        if (score < 75): continue\n",
    "        start = i\n",
    "        \n",
    "        if (len(idiom) == 1): \n",
    "            end = i + 1\n",
    "            completed = True\n",
    "            break\n",
    "            \n",
    "        for j in range(i+1, len(text)):\n",
    "            score = fuzz.ratio(text[j].lower(), idiom[j-i].lower())\n",
    "            if (score < 75): break\n",
    "            \n",
    "            if (j - i == len(idiom) - 1):\n",
    "                completed = True\n",
    "                end = j + 1\n",
    "                break\n",
    "            pass\n",
    "        \n",
    "        if (completed == True): break\n",
    "        pass\n",
    "    \n",
    "    # If not found, just return original text\n",
    "    if (start == 0 and end == -1): return premise\n",
    "    \n",
    "    # Split text into idiom and front/back sections\n",
    "    start_portion = text[:start]\n",
    "    end_portion = text[end:]\n",
    "    random.shuffle(start_portion)\n",
    "    random.shuffle(end_portion)\n",
    "    text = start_portion + text[start:end] + end_portion\n",
    "    return \" \".join(text).strip()\n",
    "\n",
    "# Uncomment below if we wish to replace the IEs with randomly generated strings\n",
    "# '''\n",
    "agg = manual_e + manual_ne + manual_antonyms_ne\n",
    "\n",
    "def generate_gibberish(cutoff=15, s=\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "    length = np.random.randint(cutoff)\n",
    "    text = \"\"\n",
    "    for i in range(length): text += random.choice(s)\n",
    "    return text\n",
    "\n",
    "for i in range(len(agg)):\n",
    "    idiom_temp = remove_contexts(agg[i][0], agg[i][1])[0]\n",
    "    gibb = generate_gibberish()\n",
    "    agg[i][0] = agg[i][0].replace(idiom_temp, gibb)\n",
    "    pass\n",
    "        \n",
    "# '''\n",
    "\n",
    "def train_test_split(manual_e, manual_ne, manual_antonyms_ne, magpie_e, magpie_adversarial_ne, magpie_ne, sample=True, no_context=True, shuffling=True):\n",
    "    # Balance the dataset labels if needed\n",
    "    if (sample):\n",
    "        ne_size = len(magpie_ne) + len(magpie_adversarial_ne)\n",
    "        constant = 1.0\n",
    "        idx = random.sample(range(len(magpie_e)), int(constant * ne_size))\n",
    "        \n",
    "        magpie_e1 = [magpie_e[i] for i in idx]\n",
    "        aggregate_train_dataset = magpie_e1 + magpie_ne + magpie_adversarial_ne\n",
    "    else:\n",
    "        aggregate_train_dataset = magpie_e + magpie_ne + magpie_adversarial_ne\n",
    "\n",
    "    # Construct IMPLI training dataset\n",
    "    impli_train_dataset = Dataset.from_dict({\"premises\" : [reformat_text(s[0]) for s in aggregate_train_dataset],\n",
    "                                             \"hypotheses\" : [reformat_text(s[1]) for s in aggregate_train_dataset],\n",
    "                                             \"labels\" : [s[2] for s in aggregate_train_dataset]}).shuffle(seed=42)\n",
    "    \n",
    "    # Construct IMPLI test dataset\n",
    "    if (no_context == True):\n",
    "        aggregate_test_dataset = manual_e + manual_ne + manual_antonyms_ne\n",
    "\n",
    "        # Strip the contexts\n",
    "        impli_test_dataset = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "\n",
    "        for s in aggregate_test_dataset:\n",
    "            # Remove the contexts from the samples\n",
    "            t1, t2 = remove_contexts(reformat_text(s[0]), reformat_text(s[1]))\n",
    "            impli_test_dataset[\"premises\"].append(t1)\n",
    "            impli_test_dataset[\"hypotheses\"].append(t2)\n",
    "            impli_test_dataset[\"labels\"].append(s[2])\n",
    "            pass\n",
    "\n",
    "        impli_test_dataset = Dataset.from_dict(impli_test_dataset)\n",
    "\n",
    "        # Construct individual test datasets\n",
    "        impli_test_manual_e = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "        impli_test_manual_ne = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "        impli_test_manual_antonyms_ne = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "\n",
    "        # Remove contexts from entailment samples\n",
    "        for s in manual_e:\n",
    "            t1, t2 = remove_contexts(reformat_text(s[0]), reformat_text(s[1]))\n",
    "            impli_test_manual_e[\"premises\"].append(t1)\n",
    "            impli_test_manual_e[\"hypotheses\"].append(t2)\n",
    "            impli_test_manual_e[\"labels\"].append(s[2])\n",
    "            pass\n",
    "\n",
    "        # Remove contexts from non-entailment samples\n",
    "        for s in manual_ne:\n",
    "            t1, t2 = remove_contexts(reformat_text(s[0]), reformat_text(s[1]))\n",
    "            impli_test_manual_ne[\"premises\"].append(t1)\n",
    "            impli_test_manual_ne[\"hypotheses\"].append(t2)\n",
    "            impli_test_manual_ne[\"labels\"].append(s[2])\n",
    "            pass\n",
    "\n",
    "        # Remove contexts from antonym samples\n",
    "        for s in manual_antonyms_ne:\n",
    "            t1, t2 = remove_contexts(reformat_text(s[0]), reformat_text(s[1]))\n",
    "            impli_test_manual_antonyms_ne[\"premises\"].append(t1)\n",
    "            impli_test_manual_antonyms_ne[\"hypotheses\"].append(t2)\n",
    "            impli_test_manual_antonyms_ne[\"labels\"].append(s[2])\n",
    "            pass\n",
    "\n",
    "        impli_test_manual_e = Dataset.from_dict(impli_test_manual_e)\n",
    "        impli_test_manual_ne = Dataset.from_dict(impli_test_manual_ne)\n",
    "        impli_test_manual_antonyms_ne = Dataset.from_dict(impli_test_manual_antonyms_ne)\n",
    "    \n",
    "    if (no_context == False):\n",
    "        # Construct IMPLI test dataset\n",
    "        aggregate_test_dataset = manual_e + manual_ne + manual_antonyms_ne\n",
    "        \n",
    "        if (shuffling == True):\n",
    "            impli_test_dataset = Dataset.from_dict({\"premises\" : [impli_shuffling(reformat_text(s[0]), aggregate_test_dataset.index(s)) for s in aggregate_test_dataset],\n",
    "                                         \"hypotheses\" : [reformat_text(s[1]) for s in aggregate_test_dataset],\n",
    "                                         \"labels\" : [s[2] for s in aggregate_test_dataset]})\n",
    "\n",
    "            # Construct individual test datasets\n",
    "            impli_test_manual_e = Dataset.from_dict({\"premises\" : [impli_shuffling(reformat_text(s[0]), manual_e.index(s)) for s in manual_e],\n",
    "                                                     \"hypotheses\" : [reformat_text(s[1]) for s in manual_e],\n",
    "                                                     \"labels\" : [s[2] for s in manual_e]})\n",
    "            impli_test_manual_ne = Dataset.from_dict({\"premises\" : [impli_shuffling(reformat_text(s[0]), manual_ne.index(s)) for s in manual_ne],\n",
    "                                                     \"hypotheses\" : [reformat_text(s[1]) for s in manual_ne],\n",
    "                                                     \"labels\" : [s[2] for s in manual_ne]})\n",
    "            impli_test_manual_antonyms_ne = Dataset.from_dict({\"premises\" : [impli_shuffling(reformat_text(s[0]), manual_antonyms_ne.index(s)) for s in manual_antonyms_ne],\n",
    "                                                               \"hypotheses\" : [reformat_text(s[1]) for s in manual_antonyms_ne],\n",
    "                                                               \"labels\" : [s[2] for s in manual_antonyms_ne]})\n",
    "        else:\n",
    "            impli_test_dataset = Dataset.from_dict({\"premises\" : [reformat_text(s[0]) for s in aggregate_test_dataset],\n",
    "                                                     \"hypotheses\" : [reformat_text(s[1]) for s in aggregate_test_dataset],\n",
    "                                                     \"labels\" : [s[2] for s in aggregate_test_dataset]})\n",
    "\n",
    "            # Construct individual test datasets\n",
    "            impli_test_manual_e = Dataset.from_dict({\"premises\" : [reformat_text(s[0]) for s in manual_e],\n",
    "                                                     \"hypotheses\" : [reformat_text(s[1]) for s in manual_e],\n",
    "                                                     \"labels\" : [s[2] for s in manual_e]})\n",
    "            impli_test_manual_ne = Dataset.from_dict({\"premises\" : [reformat_text(s[0]) for s in manual_ne],\n",
    "                                                     \"hypotheses\" : [reformat_text(s[1]) for s in manual_ne],\n",
    "                                                     \"labels\" : [s[2] for s in manual_ne]})\n",
    "            impli_test_manual_antonyms_ne = Dataset.from_dict({\"premises\" : [reformat_text(s[0]) for s in manual_antonyms_ne],\n",
    "                                                               \"hypotheses\" : [reformat_text(s[1]) for s in manual_antonyms_ne],\n",
    "                                                               \"labels\" : [s[2] for s in manual_antonyms_ne]})\n",
    "    \n",
    "    return impli_train_dataset, impli_test_dataset, impli_test_manual_e, impli_test_manual_ne, impli_test_manual_antonyms_ne\n",
    "\n",
    "train_dataset, test_dataset, test_manual_e, test_manual_ne, test_manual_antonyms_ne = train_test_split(manual_e, \n",
    "                                                                                                       manual_ne, \n",
    "                                                                                                       manual_antonyms_ne, \n",
    "                                                                                                       magpie_e + pie_e + semeval_e, \n",
    "                                                                                                       magpie_adversarial_ne + pie_adversarial_ne + semeval_adversarial_ne, \n",
    "                                                                                                       magpie_ne + pie_ne + semeval_ne,\n",
    "                                                                                                       no_context=False,\n",
    "                                                                                                       shuffling=False)                                                                                                 \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premises = examples[\"premises\"]\n",
    "    hypotheses = examples[\"hypotheses\"]\n",
    "    \n",
    "    return injected_tokenizer(premises, hypotheses, truncation=True)\n",
    "\n",
    "encoded_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_test = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "\n",
    "# Tokenization for individual datasets\n",
    "encoded_manual_e = test_manual_e.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_manual_ne = test_manual_ne.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_manual_antonyms_ne = test_manual_antonyms_ne.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=injected_tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUTE Auxiliary Methods\n",
    "set_logging_and_seed()\n",
    "\n",
    "def flute_construction(train, test, no_context=False):\n",
    "    flute_train = {\"premises\" : [reformat_text(s[0]) for s in train], \n",
    "                   \"hypotheses\" : [reformat_text(s[1]) for s in train], \n",
    "                   \"labels\" : [s[2] for s in train]}\n",
    "    flute_train = Dataset.from_dict(flute_train).shuffle(seed=42)\n",
    "    \n",
    "    flute_test = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    flute_test_e = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    flute_test_ne = {\"premises\" : [], \"hypotheses\" : [], \"labels\" : []}\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        s = test[i]\n",
    "        if (no_context == True):\n",
    "            premise, hypothesis = remove_contexts(s[0], s[1])\n",
    "        else:\n",
    "            premise = s[0]\n",
    "            hypothesis = s[1]\n",
    "        \n",
    "        premise = reformat_text(premise)\n",
    "        hypothesis = reformat_text(hypothesis)\n",
    "        \n",
    "        flute_test[\"premises\"].append(premise)\n",
    "        flute_test[\"hypotheses\"].append(hypothesis)\n",
    "        flute_test[\"labels\"].append(s[2])\n",
    "        \n",
    "        if (s[2] == [0.0, 1.0]):\n",
    "            flute_test_ne[\"premises\"].append(premise)\n",
    "            flute_test_ne[\"hypotheses\"].append(hypothesis)\n",
    "            flute_test_ne[\"labels\"].append(s[2])\n",
    "        else:\n",
    "            flute_test_e[\"premises\"].append(premise)\n",
    "            flute_test_e[\"hypotheses\"].append(hypothesis)\n",
    "            flute_test_e[\"labels\"].append(s[2])\n",
    "        pass\n",
    "    \n",
    "    flute_test = Dataset.from_dict(flute_test)\n",
    "    flute_test_e = Dataset.from_dict(flute_test_e)\n",
    "    flute_test_ne = Dataset.from_dict(flute_test_ne)\n",
    "    \n",
    "    return flute_train, flute_test, flute_test_e, flute_test_ne\n",
    "\n",
    "flute_train, flute_test, flute_test_e, flute_test_ne = flute_construction(flute_train_final, \n",
    "                                                                          flute_test_final, \n",
    "                                                                          no_context=False)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    premises = examples[\"premises\"]\n",
    "    hypotheses = examples[\"hypotheses\"]\n",
    "    \n",
    "    return injected_tokenizer(premises, hypotheses, truncation=True)\n",
    "\n",
    "encoded_flute_train = flute_train.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_flute_test = flute_test.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "\n",
    "# Tokenization for individual datasets\n",
    "encoded_flute_e = flute_test_e.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_flute_ne = flute_test_ne.map(tokenize_function, batched=True, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=injected_tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b57b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Auxiliary Methods\n",
    "set_logging_and_seed()\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels = [abs(label[0] - 1) for label in labels]\n",
    "    return metric.compute(predictions=list(predictions), references=list(labels))\n",
    "\n",
    "# Uncomment below for hyperparameter search\n",
    "# '''\n",
    "def model_init(trial): return final_model\n",
    "\n",
    "trainer = Trainer(model=None,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_split,\n",
    "                  eval_dataset=validation_split,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=injected_tokenizer,\n",
    "                  model_init=model_init,\n",
    "                  data_collator=data_collator)\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64])}\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(direction=\"maximize\",\n",
    "                                           backend=\"optuna\",\n",
    "                                           hp_space=optuna_hp_space,\n",
    "                                           n_trials=5)\n",
    "print(best_trial)\n",
    "# '''\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(output_dir=\"~/path/\",\n",
    "                                         evaluation_strategy=\"no\",\n",
    "                                         num_train_epochs=5,\n",
    "                                         learning_rate=2e-5,\n",
    "                                         weight_decay=0.01, \n",
    "                                         per_device_train_batch_size=8,\n",
    "                                         per_device_eval_batch_size=8,\n",
    "                                         fp16=True, \n",
    "                                         push_to_hub=False)\n",
    "\n",
    "trainer = Seq2SeqTrainer(final_model,\n",
    "                         training_args,\n",
    "                         train_dataset=encoded_train, \n",
    "                         eval_dataset=encoded_test,\n",
    "                         data_collator=data_collator,\n",
    "                         tokenizer=injected_tokenizer,\n",
    "                         compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(encoded_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp [~/.conda/envs/torch-nlp/]",
   "language": "python",
   "name": "conda_torch-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
