{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a9eb4-986d-47e7-a3d2-4f4398ac7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import Statements\n",
    "##################################################\n",
    "import context_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99df49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load Model\n",
    "##################################################\n",
    "context_utils.set_logging_and_seed()\n",
    "\n",
    "model, tokenizer = context_utils.load_model(model_path=\"/scratch/gpfs/kc4642/Models/bart-base\", \n",
    "                                            tokenizer_path=\"/scratch/gpfs/kc4642/Tokenizers/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bd524-9b20-42ab-beda-8b76742594e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load Dataset\n",
    "##################################################\n",
    "context_utils.set_logging_and_seed()\n",
    "\n",
    "train_ent, train_nent, train_ant, test_ent, test_nent, test_ant = context_utils.load_data(dataset_dir=\"IMPLI_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfe457-6187-4cdc-b014-671e58cca008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Construct Dataset (Part 1)\n",
    "##################################################\n",
    "context_utils.set_logging_and_seed()\n",
    "\n",
    "final_train, final_test, final_ent, final_nent, final_ant = context_utils.construct_impli(train_ent=train_ent, \n",
    "                                                                                          train_nent=train_nent, \n",
    "                                                                                          train_ant=train_ant, \n",
    "                                                                                          test_ent=test_ent, \n",
    "                                                                                          test_nent=test_nent, \n",
    "                                                                                          test_ant=test_ant, \n",
    "                                                                                          balance=True, \n",
    "                                                                                          no_context=True, \n",
    "                                                                                          shuffling=False, \n",
    "                                                                                          seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126cd77-7260-4a6f-b4dc-5b700573e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Construct Dataset (Part 2)\n",
    "##################################################\n",
    "context_utils.set_logging_and_seed()\n",
    "\n",
    "encoded_train = final_train.map(context_utils.tokenize_text, batched=True, fn_kwargs={\"tokenizer\" : tokenizer}, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_test = final_test.map(context_utils.tokenize_text, batched=True, fn_kwargs={\"tokenizer\" : tokenizer}, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_ent = final_ent.map(context_utils.tokenize_text, batched=True, fn_kwargs={\"tokenizer\" : tokenizer}, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_nent = final_nent.map(context_utils.tokenize_text, batched=True, fn_kwargs={\"tokenizer\" : tokenizer}, remove_columns=[\"premises\", \"hypotheses\"])\n",
    "encoded_ant = final_ant.map(context_utils.tokenize_text, batched=True, fn_kwargs={\"tokenizer\" : tokenizer}, remove_columns=[\"premises\", \"hypotheses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1ed7f-500b-4b40-88c2-fee004dd4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Evaluate Model\n",
    "##################################################\n",
    "context_utils.set_logging_and_seed()\n",
    "\n",
    "final_model = context_utils.trainNtest(model=model, \n",
    "                                       tokenizer=tokenizer, \n",
    "                                       training_args=dict(), \n",
    "                                       train_dataset=encoded_train, \n",
    "                                       test_datasets=[encoded_test],\n",
    "                                       skip_train=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp-backup [~/.conda/envs/torch-nlp-backup/]",
   "language": "python",
   "name": "conda_torch-nlp-backup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
